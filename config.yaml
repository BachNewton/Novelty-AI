# Snake AI Training Configuration

# Game Settings
game:
  grid_width: 20
  grid_height: 20

# Training Hyperparameters
# See EXPERIMENTS.md for rationale behind these values
training:
  episodes: 10000
  batch_size: 64
  learning_rate: 0.001
  gamma: 0.99  # Discount factor

  # Epsilon (exploration) - fast decay reaches low exploration by ~1000 episodes
  epsilon_start: 1.0
  epsilon_min: 0.01
  epsilon_decay: 0.995

  # Experience replay
  buffer_size: 100000
  target_update_freq: 100

  # Double DQN reduces Q-value overestimation
  use_double_dqn: true

  # Checkpointing
  save_interval: 100  # Save model every N episodes
  checkpoint_dir: "models"

# Reward shaping (see EXPERIMENTS.md for why most are disabled)
rewards:
  food: 10.0
  death: -10.0
  step_penalty: -0.01        # Small penalty encourages efficiency
  approach_food: 0.0         # Disabled
  retreat_food: 0.0          # Disabled
  length_bonus_factor: 0.0   # Disabled

# Device Settings
device:
  # Options: "auto", "cuda", "directml", "cpu"
  preferred: "auto"
  force_cpu: false

# Visualization Settings
visualization:
  enabled: true
  show_game: true
  show_charts: true
  render_fps: 30
  chart_update_interval: 25  # Update charts every 25 episodes (reduces lag)

  # Dashboard window
  window_width: 1400
  window_height: 900

# Replay Settings
replay:
  enabled: true
  save_dir: "replays"
  playback_fps: 10  # Slower than training for watchability
  max_replays: 10   # Keep only the best N replays

# Hardware Monitor Settings
hardware_monitor:
  enabled: true
  update_interval: 1.0  # seconds

# Logging
logging:
  level: "INFO"
  log_file: "logs/training.log"
